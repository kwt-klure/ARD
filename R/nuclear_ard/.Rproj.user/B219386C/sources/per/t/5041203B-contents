## Set working directory. Please change accordingly.
setwd("/users/halidaee/Documents/ARD_note")

## Load necessary packages.

library(parallel)
library(rdist)
library(tidyverse)
library(nuclearARD)

## Set system parameters.
# Set seed for randomization.
set.seed(125)
# Define how many cores to use on each Quest node.
num_cores <- parallel::detectCores() - 1
# Pull job array index from Slurm. This will be used to determine which parameters are run on this node for our grid search. 
job_array_index <- 1

## Define our grid. Further, to determine parameters easily from job_array_index, compute length of each dimension of grid.
# Number of individuals in the network.
N_grid        <- seq(50, 400, 50)
N_grid.length <- length(N_grid)
# Number of covariates used for ARD.
K_grid        <- seq(3, 12)
K_grid.length <- length(K_grid)
# Number of Monte Carlo simulations.
R_grid        <- seq(5, 10, 2)
R_grid.length <- length(R_grid)

# Grid for lambda is a function of K and N. Thus, it cannot be defined here. Must be defined after we know which (N, R, K)
# tuple this node is running.

## Define parameters for this node.
# Index for K is the top level index. Determine that first.
K_index                        <- floor(job_array_index/(N_grid.length * R_grid.length)) + 1
# Next is the R index. We need to "erase" the K index to back this out. 
job_array_index.without_K      <- job_array_index - ((K_index - 1) * (N_grid.length * R_grid.length))
R_index                        <- floor(job_array_index.without_K/N_grid.length) + 1
# Index for N is just what remains of the index after subtracting everything else.
job_array_index.without_K_or_N <- job_array_index.without_K - ((R_index - 1) * N_grid.length)
N_index                        <- job_array_index.without_K_or_N

# Use these indices to pull the actual (N, R, K) combination to be evaluated.
#N <- N_grid[N_index]
#K <- K_grid[K_index]
#R <- R_grid[R_index]

R <- 5
N <- 800
K <- 3

# Now, we define the  upper limit for our bandwidth grid. We derived the analytical optimal bandwidth from Negahban and Wainwright, 
# but think it is too conservative in practice. To find the actual optimal, we need to search over all bandwidth values below it.
lambda <- 2*(2*sqrt(N))*(sqrt(N) + sqrt(K))

## For the sake of minimizing code redundancy, this function standardizes the process of taking in the data from a network generating process
## and converts it to an adjacency matrix.
Adj_Matrix_Construction <- function(P_mat) {
    diag(P_mat) <- 0
    U           <- matrix(runif(N*N), nrow=N, ncol=N)
    U           <- t(U)/2 + U/2
    A_mat       <- U < P_mat
    return(list(P_mat = P_mat, A_mat = A_mat)) 
}


## To run the Monte Carlo simulation efficiently, we will define a function that will run each individual simulation.
## We will then run this function R times, using mclapply, in order to do it in parallel. 

MSE_simulation <- function(sim_number, lambda) {
    # First, we conduct the simulation for the latent space model.
    alpha        <- rnorm(N) # Generate intercept term.
    positions    <- matrix(rexp(N * 2), nrow=N, ncol=2) # Generate locations.
    latent_index <- alpha + t(replicate(N, alpha)) - pdist(positions) # Compute latent index.  
    P_LSM        <- exp(latent_index) / (1 + exp(latent_index)) # Compute log odds ratio.
    LSM_mats     <- Adj_Matrix_Construction(P_LSM)
    P_LSM        <- LSM_mats$P_mat
    A_LSM        <- LSM_mats$A_mat
    
    
    # Next, conduct simulation for random dot product graph.
    positions    <- sqrt(runif(N))
    P_RDP        <- positions * t(replicate(N, positions))
    RDP_mats     <- Adj_Matrix_Construction(P_RDP)
    P_RDP        <- RDP_mats$P_mat
    A_RDP        <- RDP_mats$A_mat


    # Third, the stochastic block model simulation.
    P_SBM        <- matrix(rep(0.3, N*N), nrow=N, ncol=N)
    groups       <- 5
    bs <- floor(N/5)
    for (g in 0:(groups - 1)) {
        P_SBM[(g * bs + 1):((g + 1) * bs), (g * bs + 1):((g + 1) * bs)] <- 0.7
    }
    SBM_mats     <- Adj_Matrix_Construction(P_SBM)
    P_SBM        <- SBM_mats$P_mat
    A_SBM        <- SBM_mats$A_mat

    # Generate ARDs[]
    types        <- matrix(rbinom(K*N, 1.0, 0.5), nrow = K, ncol = N)
    ARD_LSM      <- types %*% A_LSM
    ARD_RDP      <- types %*% A_RDP
    ARD_SBM      <- types %*% A_SBM



    # Retrieve estimated matrices.    
    P_LSM.hat <- accel_nuclear_gradient(types, ARD_LSM, lambda=lambda)
    P_RDP.hat <- accel_nuclear_gradient(types, ARD_RDP, lambda=lambda)
    P_SBM.hat <- accel_nuclear_gradient(types, ARD_SBM, lambda=lambda)


    MSE_LSM <- mean((P_LSM.hat - P_LSM)^2)
    MSE_RDP <- mean((P_RDP.hat - P_RDP)^2)
    MSE_SBM <- mean((P_SBM.hat - P_SBM)^2)

    return(list(MSE_LSM = MSE_LSM, MSE_RDP = MSE_RDP, MSE_SBM = MSE_SBM))

}

## We will now create a wrapper function for running our Monte Carlo simulations.

parallel_simulations <- function(index) {
    # First, convert lambda index into an actual lambda.

    # Next, run the simulations.
    simulation_outputs_list <- mclapply(1:R, function(x) MSE_simulation(x, lambda), mc.cores = num_cores)
    simulation_outputs <- data.frame(do.call("rbind", simulation_outputs_list)) 
        

    # Place simulation results into a dataframe.
    MSE_df <- data.frame(model.type = c('LSM', 'RDP', 'SBM'),
                         MSE.vals = c(simulation_outputs$MSE_LSM, simulation_outputs$MSE_RDP, simulation_outputs$MSE_SBM),
                         lambda.vals = rep(lambda, 3))

    # Return the dataframe as output.
    return(MSE_df)
}

## Run simulation for each possible lambda.
#length(lambda)
merged_MSE_list <- lapply(1:2, parallel_simulations)

merged_MSE_df <- merged_MSE_list %>%
                    bind_rows() %>%
                    mutate(K.vals = K) %>%
                    mutate(N.vals = N)

## Create string for name of csv file this will be exported to.
filename <- paste0('MC_output/MSE_', K, '_', R, '_', N, '.csv')

write_csv(merged_MSE_df, filename)
