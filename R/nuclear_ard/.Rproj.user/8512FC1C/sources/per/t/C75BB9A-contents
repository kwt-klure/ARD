#' Accelerated gradient function for ARD
#' 
#' \code{accel_nuclear_gradient} implements the Accelerated Gradient Algorithm (Algorithm 2) 
#' from Ji & Ye (2009). This should be the only function users interact with directly 
#' from the \emph{ardlasso} package.
#' 
#' @param inputs A matrix object. This contains ARD census data in matrix form.
#'  It should be of dimension N x K, where N = village size and K = number of ARD characteristics.
#' @param outputs A matrix object. This contains the ARD survey data 
#'  in matrix form with size M x K, where N > M = number of households receiving ARD questionairre. 
#' @param lambda A scalar (numeric) value. This is an initial guess that will be iterated on.
#' @param iterations A scalar (integer) value. It is the number of iterations the user 
#'  specifies should occur. Set by default to 100.
#' @return A scalar value denoting the value of our loss function. 
#' @export
#' @import Matrix
accel_nuclear_gradient <- function(inputs, outputs, lambda, iterations = 100, Z_1 = 0, gamma = 2.0) {
  # This function implements Algorithm 2 from "An Accelerated Gradient
  # Method for Trace Norm Minimization" by Ji & Ye (2009)
    
  # Initialize scalar values
  L       <- 1.0
  alpha   <- 1.0
  

  
  # Initialize W_0 = Z_1 \in R^{m x n}
  N <- dim(inputs)[1] 
  M <- dim(outputs)[1] 
  K <- dim(outputs)[2] 
  if (any(c('matrix', 'Matrix', 'dgCMatrix') %in% class(Z_1))) {
    if (dim(Z_1) == c(M, N)) {
      Z <- Z_1
    } else {
      Z <- runif(M * N, min = 0.0, max = 1.0) %>% 
            matrix(nrow = M, ncol = N)
    }
  } else {
    Z <- runif(M * N, min = 0.0, max = 1.0) %>% 
          matrix(nrow = M, ncol = N)
  } 
  W <- Z
  
  for (i in 1:iterations) {
    print(paste0("Step ", i, " starting at: ", Sys.time(), ". L size ", L))
    # Step 1 of iteration: Must initialize L bar by setting to L_{k - 1}
    L.bar <- L

    # Update W
    pLZ <- next_W_func(inputs, outputs, lambda, L.bar, Z)

    
    # Step 2: While loop only activated if 
    # F(p_L(Z_{k - 1})) > Q(p_L(Z_{k-1}), Z_{k-1}).
    # Removing \lambda ||W||_* from both sides since cancels out, 
    # nuclear norm computation isn't super efficient, and don't
    # calculate F or Q elsewhere, so don't need generality.
    # Effectively, this clause says only keep iterating if actual
    # loss value is still greater than approximated loss, because
    # that means we can do better by iterating on the value of L.
    F.value <- obj_func(inputs, outputs, pLZ)
    Q.value <- obj_func.approx(inputs, outputs, Z, pLZ, L.bar)
    print(paste0("Difference is ", F.value - Q.value, ": ", F.value, " vs ", Q.value))
    j <- 1
    while (F.value > Q.value) {
      print(paste0("While step ", j, ". Difference is ", F.value - Q.value, ": ", F.value, " vs ", Q.value))
      # Update L bar, as stated in algorithm.
      L.bar <- gamma * L.bar
      
      # Recalculate values for while loop check.
      pLZ     <- next_W_func(inputs, outputs, lambda, L.bar, Z)
      F.value <- obj_func(inputs, outputs, pLZ)
      Q.value <- obj_func.approx(inputs, outputs, Z, pLZ, L.bar)
      j <- j + 1
    }
    print("Done with while loop.")

    # Step 3: Update values before next iteration. 
    L           <- L.bar
    W_kmin1     <- W
    W           <- next_W_func(inputs, outputs, lambda, L, Z)
    alpha_kmin1 <- alpha
    alpha       <- (1.0 + sqrt(1.0 + 4.0*alpha^2))/2.0
    Z           <- W_kmin1 + ((alpha_kmin1 - 1.0)/alpha)*(W - W_kmin1)
    
  }
  

  
  return(W)
}