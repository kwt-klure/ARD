alpha        <- rnorm(N) # Generate intercept term.
positions    <- matrix(rexp(N * 2), nrow=N, ncol=2) # Generate locations.
latent_index <- alpha + t(replicate(N, alpha)) - pdist(positions) # Compute latent index.
P_LSM        <- exp(latent_index) / (1 + exp(latent_index)) # Compute log odds ratio.
LSM_mats     <- Adj_Matrix_Construction(P_LSM)
P_LSM        <- LSM_mats$P_mat
A_LSM        <- LSM_mats$A_mat
# Next, conduct simulation for random dot product graph.
positions    <- sqrt(runif(N))
P_RDP        <- positions * t(replicate(N, positions))
RDP_mats     <- Adj_Matrix_Construction(P_RDP)
P_RDP        <- RDP_mats$P_mat
A_RDP        <- RDP_mats$A_mat
# Third, the stochastic block model simulation.
P_SBM        <- matrix(rep(0.3, N*N), nrow=N, ncol=N)
groups       <- 5
bs <- floor(N/5)
for (g in 0:(groups - 1)) {
P_SBM[(g * bs + 1):((g + 1) * bs), (g * bs + 1):((g + 1) * bs)] <- 0.7
}
SBM_mats     <- Adj_Matrix_Construction(P_SBM)
P_SBM        <- SBM_mats$P_mat
A_SBM        <- SBM_mats$A_mat
# Generate ARDs[]
types        <- matrix(rbinom(K*N, 1.0, 0.5), nrow = K, ncol = N)
ARD_LSM      <- types %*% A_LSM
ARD_RDP      <- types %*% A_RDP
ARD_SBM      <- types %*% A_SBM
accel_nuclear_gradient(types, ARD_LSM, lambda=lambda)
library(nuclearARD)
install.packages('nuclearARD_0.1.tar.gz', repos=NULL, type='source') #Only needs to be run very first time
setwd("~/Dropbox/Projects/ARD/github_repository/R")
install.packages('nuclearARD_0.1.tar.gz', repos=NULL, type='source') #Only needs to be run very first time
R.Version()
packageVersion("Matrix")
install.packages("Matrix")
packageVersion("Matrix")
library(Matrix())
library(Matrix
)
install.packages(Matrix)
install.packages("Matrix")
library("Matrix")
install.packages("Matrix")
library("Matrix")
installed.packages("Matrix")
install.packages("Matrix")
librar("Matrix")
library("Matrix")
setwd("~/Dropbox/Projects/ARD/github_repository/R")
setwd("/private/var/folders/f1/tfkz7_y16t1dfwc0zv1r43vc0000gn/T/Rtmp3vz8e1/downloaded_packages")
library("Matrix")
install.packages("Matrix")
library("Matrix")
install.packages('nuclearARD_0.1.tar.gz', repos=NULL, type='source') #Only needs to be run very first time
library(nuclearARD)
set.seed(0) # set seed
N1 <- 100
N2 <- 200
K <- as.integer(round(N1^0.4))
# simulate network
positions <- sqrt(runif(N2))
M <- positions %*% t(positions) # n x n matrix of link probabilities
diag(M) <- 0 # zero out diagonal entries to ensure no self links
U <- matrix(runif(N2^2), nrow=N2, ncol=N2)
U <- t(U)/2 + U/2 # make matrix symmetric to have an undirected network
G <- (U < M)[,1:N1] # simulated network submatrix
types <- matrix(rbinom(K*N2, size=1, prob=0.5), nrow=K, ncol=N2)
ARDs <- types %*% G
write.csv(t(ARDs), 'ARD_data.csv')
write.csv(t(types), 'type_data.csv')
# load CSVs as numpy matrices
ARDs <- t(as.matrix(read.csv('ARD_data.csv')))
types <- t(as.matrix(read.csv('type_data.csv')))
# store dimensions
K <- nrow(ARDs)
N1 <- ncol(ARDs)
N2 <- ncol(types)
lmbd <- 2 * (sqrt(N1) + sqrt(N2)) * (sqrt(N2) + sqrt(K))
M_hat <- accel_nuclear_gradient(types, ARDs, lmbd)
#or just run
M_hat <- matrix_regression(types, ARDs)
print(M_hat[1:5, 1:5])
write.csv(as.matrix(M_hat), 'estimated_network.csv')
U <- matrix(runif(N2*N1), nrow=N2, ncol=N1) # draw uniform random variables
diag(U) <- 0 # zero out the diagonal entries (if no self links)
U_sub <- U[1:N1, ] # extract upper N1 x N1 submatrix
U_sub <- t(U_sub)/2 + U_sub # symmetrize the submatrix
U[1:N1, ] <- U_sub # replace the upper N1 x N1 submatrix of
#     the original matrix U with U_sub
G <- 1*(U < M_hat)
print(G[1:10, 1:10])
install.packages('nuclearARD_0.1.tar.gz', repos=NULL, type='source') #Only needs to be run very first time
setwd("~/Dropbox/Projects/ARD/github_repository/R")
install.packages('nuclearARD_0.1.tar.gz', repos=NULL, type='source') #Only needs to be run very first time
library(nuclearARD)
set.seed(0) # set seed
N1 <- 100
N2 <- 200
K <- as.integer(round(N1^0.4))
# simulate network
positions <- sqrt(runif(N2))
M <- positions %*% t(positions) # n x n matrix of link probabilities
diag(M) <- 0 # zero out diagonal entries to ensure no self links
U <- matrix(runif(N2^2), nrow=N2, ncol=N2)
U <- t(U)/2 + U/2 # make matrix symmetric to have an undirected network
G <- (U < M)[,1:N1] # simulated network submatrix
types <- matrix(rbinom(K*N2, size=1, prob=0.5), nrow=K, ncol=N2)
ARDs <- types %*% G
write.csv(t(ARDs), 'ARD_data.csv')
write.csv(t(types), 'type_data.csv')
# load CSVs as numpy matrices
ARDs <- t(as.matrix(read.csv('ARD_data.csv')))
types <- t(as.matrix(read.csv('type_data.csv')))
# store dimensions
K <- nrow(ARDs)
N1 <- ncol(ARDs)
N2 <- ncol(types)
lmbd <- 2 * (sqrt(N1) + sqrt(N2)) * (sqrt(N2) + sqrt(K))
M_hat <- accel_nuclear_gradient(types, ARDs, lmbd)
#or just run
M_hat <- matrix_regression(types, ARDs)
print(M_hat[1:5, 1:5])
write.csv(as.matrix(M_hat), 'estimated_network.csv')
U <- matrix(runif(N2*N1), nrow=N2, ncol=N1) # draw uniform random variables
diag(U) <- 0 # zero out the diagonal entries (if no self links)
U_sub <- U[1:N1, ] # extract upper N1 x N1 submatrix
U_sub <- t(U_sub)/2 + U_sub # symmetrize the submatrix
U[1:N1, ] <- U_sub # replace the upper N1 x N1 submatrix of
#     the original matrix U with U_sub
G <- 1*(U < M_hat)
print(G[1:10, 1:10])
set.seed(0) # set seed
N1 <- 100
N2 <- 200
K <- as.integer(round(N1^0.4))
# simulate network
positions <- sqrt(runif(N2))
M <- positions %*% t(positions) # n x n matrix of link probabilities
diag(M) <- 0 # zero out diagonal entries to ensure no self links
U <- matrix(runif(N2^2), nrow=N2, ncol=N2)
U <- t(U)/2 + U/2 # make matrix symmetric to have an undirected network
G <- (U < M)[,1:N1] # simulated network submatrix
print(G[1:10, 1:10])
types <- matrix(rbinom(K*N2, size=1, prob=0.5), nrow=K, ncol=N2)
ARDs <- types %*% G
write.csv(t(ARDs), 'ARD_data.csv')
write.csv(t(types), 'type_data.csv')
# load CSVs as numpy matrices
ARDs <- t(as.matrix(read.csv('ARD_data.csv')))
types <- t(as.matrix(read.csv('type_data.csv')))
# store dimensions
K <- nrow(ARDs)
N1 <- ncol(ARDs)
N2 <- ncol(types)
lmbd <- 2 * (sqrt(N1) + sqrt(N2)) * (sqrt(N2) + sqrt(K))
M_hat <- accel_nuclear_gradient(types, ARDs, lmbd)
#or just run
M_hat <- matrix_regression(types, ARDs)
print(M_hat[1:5, 1:5])
write.csv(as.matrix(M_hat), 'estimated_network.csv')
U <- matrix(runif(N2*N1), nrow=N2, ncol=N1) # draw uniform random variables
diag(U) <- 0 # zero out the diagonal entries (if no self links)
U_sub <- U[1:N1, ] # extract upper N1 x N1 submatrix
U_sub <- t(U_sub)/2 + U_sub # symmetrize the submatrix
U[1:N1, ] <- U_sub # replace the upper N1 x N1 submatrix of
#     the original matrix U with U_sub
G <- 1*(U < M_hat)
print(G[1:10, 1:10])
print(M[1:10, 1:10])
print(G[1:5, 1:5])
print(M[1:5, 1:5])
print(M_hat[1:5, 1:5])
cor(M,M_hat)
cor(as.vecotr(M),as.vector(M_hat))
cor(as.vector(M),as.vector(M_hat))
dim(M)
dim(M_hat)
print(M[1:5, 1:5])
print(M_hat[1:5, 1:5])
K
N1 <- 100
N2 <- 200
K <- as.integer(round(N1))
# simulate network
positions <- sqrt(runif(N2))
M <- positions %*% t(positions) # n x n matrix of link probabilities
diag(M) <- 0 # zero out diagonal entries to ensure no self links
U <- matrix(runif(N2^2), nrow=N2, ncol=N2)
U <- t(U)/2 + U/2 # make matrix symmetric to have an undirected network
G <- (U < M)[,1:N1] # simulated network submatrix
types <- matrix(rbinom(K*N2, size=1, prob=0.5), nrow=K, ncol=N2)
ARDs <- types %*% G
write.csv(t(ARDs), 'ARD_data.csv')
write.csv(t(types), 'type_data.csv')
# load CSVs as numpy matrices
ARDs <- t(as.matrix(read.csv('ARD_data.csv')))
types <- t(as.matrix(read.csv('type_data.csv')))
# store dimensions
K <- nrow(ARDs)
N1 <- ncol(ARDs)
N2 <- ncol(types)
lmbd <- 2 * (sqrt(N1) + sqrt(N2)) * (sqrt(N2) + sqrt(K))
M_hat <- accel_nuclear_gradient(types, ARDs, lmbd)
100^.4
100^.5
100^.9
N1 <- 100
N2 <- 200
K <- 20
# simulate network
positions <- sqrt(runif(N2))
M <- positions %*% t(positions) # n x n matrix of link probabilities
diag(M) <- 0 # zero out diagonal entries to ensure no self links
U <- matrix(runif(N2^2), nrow=N2, ncol=N2)
U <- t(U)/2 + U/2 # make matrix symmetric to have an undirected network
G <- (U < M)[,1:N1] # simulated network submatrix
types <- matrix(rbinom(K*N2, size=1, prob=0.5), nrow=K, ncol=N2)
ARDs <- types %*% G
write.csv(t(ARDs), 'ARD_data.csv')
write.csv(t(types), 'type_data.csv')
# load CSVs as numpy matrices
ARDs <- t(as.matrix(read.csv('ARD_data.csv')))
types <- t(as.matrix(read.csv('type_data.csv')))
# store dimensions
K <- nrow(ARDs)
N1 <- ncol(ARDs)
N2 <- ncol(types)
lmbd <- 2 * (sqrt(N1) + sqrt(N2)) * (sqrt(N2) + sqrt(K))
M_hat <- accel_nuclear_gradient(types, ARDs, lmbd)
#or just run
M_hat <- matrix_regression(types, ARDs)
print(M_hat[1:5, 1:5])
write.csv(as.matrix(M_hat), 'estimated_network.csv')
U <- matrix(runif(N2*N1), nrow=N2, ncol=N1) # draw uniform random variables
diag(U) <- 0 # zero out the diagonal entries (if no self links)
U_sub <- U[1:N1, ] # extract upper N1 x N1 submatrix
U_sub <- t(U_sub)/2 + U_sub # symmetrize the submatrix
U[1:N1, ] <- U_sub # replace the upper N1 x N1 submatrix of
#     the original matrix U with U_sub
G <- 1*(U < M_hat)
print(G[1:10, 1:10])
print(M[1:5, 1:5])
print(M_hat[1:5, 1:5])
print(G[1:10, 1:10])
## Set working directory. Please change accordingly.
setwd("/users/halidaee/Documents/ARD_note")
## Load necessary packages.
library(parallel)
library(rdist)
library(tidyverse)
## Set system parameters.
# Set seed for randomization.
set.seed(125)
# Define how many cores to use on each Quest node.
num_cores <- parallel::detectCores() - 1
# Pull job array index from Slurm. This will be used to determine which parameters are run on this node for our grid search.
job_array_index <- 1
## Define our grid. Further, to determine parameters easily from job_array_index, compute length of each dimension of grid.
# Number of individuals in the network.
N_grid        <- seq(50, 400, 50)
N_grid.length <- length(N_grid)
# Number of covariates used for ARD.
K_grid        <- seq(3, 12)
K_grid.length <- length(K_grid)
# Number of Monte Carlo simulations.
R_grid        <- seq(5, 10, 2)
R_grid.length <- length(R_grid)
# Grid for lambda is a function of K and N. Thus, it cannot be defined here. Must be defined after we know which (N, R, K)
# tuple this node is running.
## Define parameters for this node.
# Index for K is the top level index. Determine that first.
K_index                        <- floor(job_array_index/(N_grid.length * R_grid.length)) + 1
# Next is the R index. We need to "erase" the K index to back this out.
job_array_index.without_K      <- job_array_index - ((K_index - 1) * (N_grid.length * R_grid.length))
R_index                        <- floor(job_array_index.without_K/N_grid.length) + 1
# Index for N is just what remains of the index after subtracting everything else.
job_array_index.without_K_or_N <- job_array_index.without_K - ((R_index - 1) * N_grid.length)
N_index                        <- job_array_index.without_K_or_N
# Use these indices to pull the actual (N, R, K) combination to be evaluated.
#N <- N_grid[N_index]
#K <- K_grid[K_index]
#R <- R_grid[R_index]
R <- 5
N <- 800
K <- 3
# Now, we define the  upper limit for our bandwidth grid. We derived the analytical optimal bandwidth from Negahban and Wainwright,
# but think it is too conservative in practice. To find the actual optimal, we need to search over all bandwidth values below it.
lambda <- 2*(2*sqrt(N))*(sqrt(N) + sqrt(K))
## For the sake of minimizing code redundancy, this function standardizes the process of taking in the data from a network generating process
## and converts it to an adjacency matrix.
Adj_Matrix_Construction <- function(P_mat) {
diag(P_mat) <- 0
U           <- matrix(runif(N*N), nrow=N, ncol=N)
U           <- t(U)/2 + U/2
A_mat       <- U < P_mat
return(list(P_mat = P_mat, A_mat = A_mat))
}
## To run the Monte Carlo simulation efficiently, we will define a function that will run each individual simulation.
## We will then run this function R times, using mclapply, in order to do it in parallel.
MSE_simulation <- function(sim_number, lambda) {
# First, we conduct the simulation for the latent space model.
alpha        <- rnorm(N) # Generate intercept term.
positions    <- matrix(rexp(N * 2), nrow=N, ncol=2) # Generate locations.
latent_index <- alpha + t(replicate(N, alpha)) - pdist(positions) # Compute latent index.
P_LSM        <- exp(latent_index) / (1 + exp(latent_index)) # Compute log odds ratio.
LSM_mats     <- Adj_Matrix_Construction(P_LSM)
P_LSM        <- LSM_mats$P_mat
A_LSM        <- LSM_mats$A_mat
# Next, conduct simulation for random dot product graph.
positions    <- sqrt(runif(N))
P_RDP        <- positions * t(replicate(N, positions))
RDP_mats     <- Adj_Matrix_Construction(P_RDP)
P_RDP        <- RDP_mats$P_mat
A_RDP        <- RDP_mats$A_mat
# Third, the stochastic block model simulation.
P_SBM        <- matrix(rep(0.3, N*N), nrow=N, ncol=N)
groups       <- 5
bs <- floor(N/5)
for (g in 0:(groups - 1)) {
P_SBM[(g * bs + 1):((g + 1) * bs), (g * bs + 1):((g + 1) * bs)] <- 0.7
}
SBM_mats     <- Adj_Matrix_Construction(P_SBM)
P_SBM        <- SBM_mats$P_mat
A_SBM        <- SBM_mats$A_mat
# Generate ARDs[]
types        <- matrix(rbinom(K*N, 1.0, 0.5), nrow = K, ncol = N)
ARD_LSM      <- types %*% A_LSM
ARD_RDP      <- types %*% A_RDP
ARD_SBM      <- types %*% A_SBM
# Retrieve estimated matrices.
P_LSM.hat <- accel_nuclear_gradient(types, ARD_LSM, lambda=lambda)
P_RDP.hat <- accel_nuclear_gradient(types, ARD_RDP, lambda=lambda)
P_SBM.hat <- accel_nuclear_gradient(types, ARD_SBM, lambda=lambda)
MSE_LSM <- mean((P_LSM.hat - P_LSM)^2)
MSE_RDP <- mean((P_RDP.hat - P_RDP)^2)
MSE_SBM <- mean((P_SBM.hat - P_SBM)^2)
return(list(MSE_LSM = MSE_LSM, MSE_RDP = MSE_RDP, MSE_SBM = MSE_SBM))
}
## We will now create a wrapper function for running our Monte Carlo simulations.
parallel_simulations <- function(index) {
# First, convert lambda index into an actual lambda.
# Next, run the simulations.
simulation_outputs_list <- mclapply(1:R, function(x) MSE_simulation(x, lambda), mc.cores = num_cores)
simulation_outputs <- data.frame(do.call("rbind", simulation_outputs_list))
# Place simulation results into a dataframe.
MSE_df <- data.frame(model.type = c('LSM', 'RDP', 'SBM'),
MSE.vals = c(simulation_outputs$MSE_LSM, simulation_outputs$MSE_RDP, simulation_outputs$MSE_SBM),
lambda.vals = rep(lambda, 3))
# Return the dataframe as output.
return(MSE_df)
}
## Run simulation for each possible lambda.
#length(lambda)
merged_MSE_list <- lapply(1:2, parallel_simulations)
merged_MSE_df <- merged_MSE_list %>%
bind_rows() %>%
mutate(K.vals = K) %>%
mutate(N.vals = N)
## Create string for name of csv file this will be exported to.
filename <- paste0('MC_output/MSE_', K, '_', R, '_', N, '.csv')
write_csv(merged_MSE_df, filename)
MSE_simulation(1, lambda)
library(nuclearARD)
MSE_simulation(1, lambda)
N_grid        <- seq(800, 1200, 100) #5 numbers
N_grid.length <- length(N_grid)
# Number of covariates used for ARD.
K_grid        <- seq(12, 36, 4) #7 numbers
K_grid.length <- length(K_grid)
N_grid.length*K_grid.length
library(tidyverse)
prod <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv')
View(prod)
unique(prod %>% select('Series Name'))
rm(prod)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
select('2016 [YR2016]') %>%
filter('Series Name' == "Employment in agriculture (% of total employment) (modeled ILO estimate)")
View(emp)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
select("Series Name", '2016 [YR2016]') %>%
filter('Series Name' == "Employment in agriculture (% of total employment) (modeled ILO estimate)")
View(emp)
library(tidyverse)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
select("Series Name", '2016 [YR2016]', 'Country Name', 'Country Code', 'Series Code') %>%
filter('Series Name' == "Employment in agriculture (% of total employment) (modeled ILO estimate)")
View(emp)
library(tidyverse)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
select("Series Name", '2016 [YR2016]', 'Country Name', 'Country Code', 'Series Code')
View(emp)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
select("Series Name", '2016 [YR2016]', 'Country Name', 'Country Code', 'Series Code') %>%
filter('Series Code' == "SL.AGR.EMPL.ZS")
View(emp)
library(tidyverse)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
select("Series Name", '2016 [YR2016]', 'Country Name', 'Country Code', 'Series Code') %>%
filter('Series Code' == "SL.AGR.EMPL.ZS
")
View(emp)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
select("Series Name", '2016 [YR2016]', 'Country Name', 'Country Code', 'Series Code') %>%
filter("SL.AGR.EMPL.ZS" %in% 'Series Code')
View(emp)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
select("Series Name", '2016 [YR2016]', 'Country Name', 'Country Code', 'Series Code') %>%
filter("EMPL.ZS" %in% 'Series Code')
View(emp)
library(tidyverse)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
select("Series Name", '2016 [YR2016]', 'Country Name', 'Country Code', 'Series Code')
View(emp)
emp[1682,]
emp[1682,]$`Series Code`
grepl('EMPL.ZS', emp$`Series Code`)
grepl('EMPL.ZS', emp$`Series Code`)[1682]
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
select("Series Name", '2016 [YR2016]', 'Country Name', 'Country Code', 'Series Code') %>%
filter(grepl("EMPL.ZS",'Series Code'))
View(emp)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
filter(grepl("EMPL.ZS",'Series Code'))
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
select("Series Name", '2016 [YR2016]', 'Country Name', 'Country Code', 'Series Code') %>%
filter(grepl("EMPL.ZS",'Series Code'))
View(emp)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
filter(grepl("EMPL.ZS",'Series Code'))
View(emp)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
filter(grepl("EMPL.ZS",'Series Code'))
View(emp)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
filter(grepl("EMPL.ZS",'Series Code'))
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
filter(grepl("EMPL.ZS",`Series Code``))
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
filter(grepl("EMPL.ZS",`Series Code``))
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
filter(grepl("EMPL.ZS",`Series Code`))
View(emp)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
filter(grepl("EMPL.ZS", `Series Code`)) %>%
select("Series Name", '2016 [YR2016]', 'Country Name', 'Country Code', 'Series Code')
View(emp)
pov <- read_csv('/users/halidaee/Downloads/share-of-the-population-living-in-extreme-poverty.csv')
View(pov)
View(emp)
pov <- read_csv('/users/halidaee/Downloads/share-of-the-population-living-in-extreme-poverty.csv') %>%
rename(Poverty = `Share of the population living in extreme poverty (%)`) %>%
group_by(Code) %>%
filter(Poverty == max(Poverty)) %>%
arrange(Entity)
View(pov)
pov <- read_csv('/users/halidaee/Downloads/share-of-the-population-living-in-extreme-poverty.csv') %>%
rename(Poverty = `Share of the population living in extreme poverty (%)`) %>%
group_by(Code) %>%
filter(Poverty == max(Poverty)) %>%
filter(Year > 2007)
arrange(Entity)
View(pov)
pov <- read_csv('/users/halidaee/Downloads/share-of-the-population-living-in-extreme-poverty.csv') %>%
rename(Poverty = `Share of the population living in extreme poverty (%)`) %>%
group_by(Entity, Code) %>%
filter(Poverty == max(Poverty)) %>%
filter(Year > 2007) %>%
arrange(Entity)
View(pov)
pov <- read_csv('/users/halidaee/Downloads/share-of-the-population-living-in-extreme-poverty.csv') %>%
rename(Poverty = `Share of the population living in extreme poverty (%)`) %>%
group_by(Entity, Code) %>%
filter(Year == max(Year)) %>%
filter(Year > 2007) %>%
arrange(Entity)
View(pov)
comb <- emp %>%
inner_join(pov, by=Code)
View(comb)
emp <- read_csv('/users/halidaee/Downloads/Data_Extract_From_World_Development_Indicators/d64b1c8e-ec51-4b8f-97e3-1d8f43d8a349_Data.csv') %>%
filter(grepl("EMPL.ZS", `Series Code`)) %>%
select('2016 [YR2016]', 'Country Name', 'Country Code') %>%
rename(Code = `Country Code`, Entity = `Country Name`, Emp = `2016 [YR2016]`)
pov <- read_csv('/users/halidaee/Downloads/share-of-the-population-living-in-extreme-poverty.csv') %>%
rename(Poverty = `Share of the population living in extreme poverty (%)`) %>%
group_by(Entity, Code) %>%
filter(Year == max(Year)) %>%
filter(Year > 2007) %>%
arrange(Entity)
comb <- emp %>%
inner_join(pov, by=Code)
View(comb)
columns(emp)
names(emp)
names(pov)
?inner_join
comb <- emp %>%
inner_join(pov)
View(comb)
cor(comb$Poverty, comb$Emp)
comb <- emp %>%
inner_join(pov) %>%
mutate(Poverty = as.numeric(Poverty)) %>%
mutate(Emp = as.numeric(Emp))
cor(comb$Poverty, comb$Emp)
View(comb)
cor(comb$Poverty, comb$Emp, na.rm = TRUE)
cor(comb$Poverty, comb$Emp, na.rm = TRUE)
?cor
cor(comb$Poverty, comb$Emp, use='pairwise.complete.obs')
